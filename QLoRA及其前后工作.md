# 模型量化的LoRA微调方法
模型量化多用来用在推理阶段节省显存，将高精度的float类型参数转变成低精度的int类型参数，牺牲一些性能但是节省大量显存。LoRA用在微调训练阶段，冻结原始模型的参数，将更新的矩阵分解为秩较低的矩阵，基于一个理论：在特定领域任务上，参数矩阵的本征秩是很低的。那么LoRA认为，参数更新的那部分秩可能也很低，那么就可以直接用一个低秩的矩阵作为原参数矩阵的更新值，微调训练时先不断更新这个低秩矩阵，结束后直接加到原来的参数矩阵中。

