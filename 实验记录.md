# 在MCTS解码方法中加入曾经代码的错误信息 模型为
暂时的结论：效果变差了，APPS数据集的严格通过率只有个位数。
1. 错误信息是上一次调用模型产生代码的错误信息  
   > 该实验是使用codegeex2_6b实现的，暂时认为变差的原因是小模型无法理解其产生的错误信息，错误信息本应该成为正反馈但是当小模型无法理解这个信息的时候，可能就变成了负反馈
  
2. 错误信息是蒙特卡洛搜索树的父节点产生代码的错误信息
    > 进行中，该实验将使用llama2_70b_GPTQ版本，利用GUANACO的prompt模板。实验效果一般，由于只进行16次树的扩展，那么很多结点的父节点没有错误信息，因此，16次扩展和生成代码的过程中，有很多次是没有错误反馈信息的。而如果大幅提高扩展的次数，那么解题会变得非常慢 
# 利用TOT改变MCTS解码过程
* 利用TOT的方式，一个想法是将MCTS树结点上的每个token变为一段token
> 因为单纯的单个token对整个代码生成可能没有帮助，MCTS进行16次最多能够判断前16个字符的效果，那么如果变成16段，每段10个token那么很可能所有的代码token都在这颗树上，就可以利用UCT算法来选择最佳片段
# 利用llama_70B组成一个软件开发过程，测试其与GPT的效果差距
# 将CoT,ToT建模成图，利用图数据去训练模型，使其思维过程在模型内部发生，不需要进行多步的生成。
# 算一下CoT或ToT完成任务时，每一步的信息增益
如果每一步信息增益都是增长的，那么说明CoT一类的方法模型每一步提供了一些额外信息，但是每一步提供的信息都有限，所以必须多步计算，最终有了足够的信息之后来解决问题。